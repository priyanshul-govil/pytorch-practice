{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshul/miniconda3/envs/learntorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "tensor([[[ 0.1629, -0.2166, -0.1410,  ..., -0.2619, -0.0819,  0.0092],\n",
      "         [ 0.4628,  0.0248, -0.0785,  ..., -0.0859,  0.5122, -0.3939],\n",
      "         [-0.0644,  0.1551, -0.6306,  ...,  0.2488,  0.3691,  0.0833],\n",
      "         ...,\n",
      "         [-0.5591, -0.4490, -1.4540,  ...,  0.1650, -0.1302, -0.3740],\n",
      "         [ 0.1400, -0.3875, -0.7916,  ..., -0.1780,  0.1824,  0.2185],\n",
      "         [ 0.1721, -0.2420, -0.1124,  ..., -0.1068,  0.1205, -0.3213]]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "token_embeddings = output.last_hidden_state\n",
    "print(token_embeddings.shape)\n",
    "# Result: token embedding shape is [1, 10, 768]\n",
    "\n",
    "print(token_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.mean(dim=1).squeeze()\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4419e-01, -6.5792e-02, -6.6249e-01,  8.4424e-02, -6.6759e-03,\n",
       "         1.5365e-01,  3.7242e+00, -2.5630e-01,  2.9933e-02, -1.7607e-01,\n",
       "         1.7791e-01, -2.1023e-01, -2.4490e-01,  5.7506e-02, -2.2517e-01,\n",
       "        -2.3024e-01, -7.9503e-03, -3.9460e-01,  3.9133e-01,  1.4674e-02,\n",
       "        -1.0418e-02, -1.7368e-01, -1.5253e-01,  4.1727e-02,  1.0535e-01,\n",
       "         1.0266e-02, -5.3535e-01, -6.1404e-02,  2.4219e-01,  2.6487e-01,\n",
       "        -9.8582e-02, -8.7298e-02, -1.1706e-01, -4.0085e-01, -2.9286e-01,\n",
       "        -4.3727e-01,  6.1490e+01,  2.2263e-01,  1.6192e-01,  3.4314e-01,\n",
       "        -3.0768e-01,  2.0968e-01,  1.7426e-01, -1.6834e-01, -3.3847e-02,\n",
       "        -2.6481e-01, -9.5405e-02, -5.4566e-01, -1.0857e-01,  9.8272e-01,\n",
       "         1.0347e-01,  2.5673e-01, -1.5698e-01,  1.5115e-01,  1.9587e-02,\n",
       "         4.2705e-01,  3.4368e-02,  4.2566e-02,  3.0398e-02, -1.1355e-01,\n",
       "         1.5174e-01,  7.8775e-02,  6.1441e-02, -1.0152e-01, -1.0989e+00,\n",
       "         9.3133e-02,  2.1198e-01, -4.3320e-01, -3.6058e-02, -2.4199e-02,\n",
       "        -8.6285e-03,  1.2822e-01, -2.1299e-01,  2.2666e-01,  6.0066e-02,\n",
       "        -3.2623e-01,  2.1745e-01, -4.1968e-01,  2.7873e-01, -9.8848e-03,\n",
       "        -7.6772e-01, -2.9891e-01,  2.0637e-01, -3.4713e-03, -3.7124e-01,\n",
       "         1.2904e-01, -1.9194e-01, -1.0216e+00,  3.3266e-01,  2.3335e-01,\n",
       "         4.4427e-02,  2.8751e-01, -2.3685e-01, -3.2212e-02, -7.1060e-02,\n",
       "        -2.3882e-02,  2.3276e-01, -2.6228e-01,  1.9589e-03,  1.8896e-01,\n",
       "         2.4273e-01,  1.0214e-01,  8.7843e-02,  1.0525e-01, -1.0816e-02,\n",
       "        -1.1230e-01, -1.5213e-01,  2.5296e+00, -7.6351e-02, -2.9856e-01,\n",
       "        -1.8957e-01, -1.8225e-01, -1.8671e-01, -7.9960e-02,  2.8139e-01,\n",
       "        -2.4210e-01, -7.4970e-02,  1.1401e-01,  1.8529e-01,  2.1506e-01,\n",
       "        -5.0796e-02,  1.3971e-01,  3.1560e-02,  1.4171e-01,  4.0281e-02,\n",
       "         7.3708e-03,  6.6668e-02, -1.5678e-01, -2.1071e-02,  1.3064e-01,\n",
       "        -3.2061e-02,  1.2589e-01,  3.6485e-01, -1.3044e-01, -3.2509e-01,\n",
       "         1.0773e-01, -6.8872e-02,  1.4125e-01,  5.0928e-01,  1.0107e-01,\n",
       "         1.4323e-01,  1.7796e-02, -1.1219e+00,  2.2926e-02,  1.5223e-01,\n",
       "         1.1225e-01, -3.0845e-01, -2.7338e-01, -8.7502e-04, -1.9994e-01,\n",
       "        -2.1147e-01, -3.7923e-01,  1.1390e-03, -1.6357e-01, -3.7830e-01,\n",
       "        -2.3387e-01, -4.6970e-01, -2.1993e-01, -5.5241e-02, -6.1328e-02,\n",
       "        -6.6670e-02, -2.9947e-01,  2.4758e-02,  7.2484e-02,  1.3340e-01,\n",
       "        -7.7771e-02, -2.5571e-02, -1.4070e+00, -9.5409e-03, -1.6353e-01,\n",
       "        -2.0260e-01, -8.4857e-02, -8.8646e-02,  2.3609e-01,  2.4393e-01,\n",
       "         2.1203e-01,  7.6792e-02, -5.9557e-02,  2.4608e-01,  2.1078e-01,\n",
       "         3.6751e-03,  1.2973e-02, -4.0626e-02, -1.8719e-01,  1.2997e-01,\n",
       "        -7.4303e-02,  1.3527e-01, -2.0716e-01,  9.0119e-02,  4.3346e-01,\n",
       "         1.8169e-01, -3.0643e-02,  2.6462e-01, -1.9073e-01, -1.8128e-01,\n",
       "        -2.6446e-01, -1.3248e-01, -1.2593e-01, -7.1728e-02, -2.6118e-01,\n",
       "        -2.5870e-02,  2.0111e-01,  1.2228e-01, -2.1305e-01,  9.3096e-02,\n",
       "        -9.0245e-02,  1.0077e-02,  1.2089e-01, -4.3146e-01, -3.2433e-02,\n",
       "         1.4622e-01, -3.2038e-02,  2.7199e-01, -3.5061e-01, -6.5229e-02,\n",
       "         1.6531e-01,  2.3496e-03, -7.2530e-02, -9.1148e-02, -1.8782e-01,\n",
       "        -1.9342e-01,  1.3651e-01, -3.4473e-01,  1.0417e-01,  2.3552e-01,\n",
       "         2.3898e-01, -8.5121e-02, -1.5513e-01, -1.2447e-01, -6.0275e-02,\n",
       "         3.0423e-01,  6.7215e-02,  7.4650e-01, -1.0857e-01,  1.1964e-01,\n",
       "        -1.6739e-01,  1.6892e-01,  3.7752e-02, -1.8351e-01,  3.6178e-02,\n",
       "        -5.3465e-02,  3.7020e-01,  2.9652e-01, -3.4593e-02,  9.4504e-02,\n",
       "         1.4390e-01, -2.0937e-01,  2.0242e-01,  5.4092e-02, -1.3647e-01,\n",
       "        -3.3991e-01, -2.1522e-01, -1.0207e-01,  2.5621e-01, -9.8280e-02,\n",
       "         5.9770e-01, -1.8449e-02,  1.5411e-01,  1.4587e-01,  1.0590e-01,\n",
       "         1.3135e-01,  1.7750e-01, -6.4502e-02,  2.7076e-01,  5.6313e-02,\n",
       "         1.2204e-01, -1.0107e+00,  1.1633e-03, -5.3936e-02,  1.4448e-01,\n",
       "         1.4981e-01,  8.0132e-01, -2.1439e-01,  4.2710e-01, -2.6214e-01,\n",
       "         5.0562e-02,  1.2633e-01, -3.6713e-02, -1.7718e-01,  1.8457e-01,\n",
       "        -9.8782e-02, -1.5570e-02,  3.5976e-01,  1.6678e-01,  1.3393e-01,\n",
       "        -4.8374e-02,  1.1234e-01, -2.0892e-01, -6.1111e-02,  3.3229e-01,\n",
       "        -5.4297e-03, -3.6875e-01, -3.9773e-01,  4.9500e-02,  1.2252e-01,\n",
       "        -3.8919e-01, -5.4756e-02,  1.0604e-02,  1.8603e-01,  2.7350e-01,\n",
       "         3.7858e-01, -8.8191e-02, -4.4260e-02,  1.2765e-01, -1.9618e-01,\n",
       "        -1.9533e-02, -2.2565e-02, -2.7449e-02, -2.1922e-01, -2.5447e-01,\n",
       "        -1.9890e-01, -4.0306e-02,  1.4951e-01, -1.0311e-01, -3.5513e+01,\n",
       "         1.2927e-01, -1.4545e-01,  9.2958e-02,  8.4607e-02,  3.7611e-01,\n",
       "        -4.3294e-01, -2.9758e-02, -1.4407e-01,  1.8473e-01,  2.7655e-02,\n",
       "        -1.6449e-01,  5.8888e-01, -1.6902e-01,  7.7562e-02, -1.2964e-01,\n",
       "        -2.7069e-01, -1.9029e-01,  4.1090e-01, -3.0427e-01,  2.0138e-01,\n",
       "        -3.3164e-01,  4.1096e-02, -2.3055e-01, -2.1543e-02,  3.8513e-02,\n",
       "        -1.9565e-01,  5.4981e-02, -2.0429e-02, -6.9709e-02,  1.4556e-02,\n",
       "        -7.2566e-02, -1.5980e-01, -2.2151e-01,  2.1038e-01, -8.3627e-02,\n",
       "        -1.2682e-01,  6.2028e-01, -7.7867e-02, -9.6908e-02,  6.1254e-01,\n",
       "         2.6278e-01, -1.2409e-01, -1.0390e-02,  1.0674e-01,  5.4722e-02,\n",
       "        -1.0852e-01,  1.6117e-02,  2.7231e-01, -4.8082e-02,  1.4553e-01,\n",
       "        -1.7381e+00,  2.0206e-01,  2.1662e-01, -4.9107e-02, -1.0288e-01,\n",
       "         4.5362e-01,  1.9355e-01, -5.2117e-01, -1.1460e+00, -1.5538e+01,\n",
       "        -1.8202e-01, -1.4271e-01,  1.1062e+00,  7.1249e-02, -2.3674e-01,\n",
       "         1.8700e-01,  1.0302e-01, -7.1511e-02, -1.2913e-01, -4.0848e-01,\n",
       "        -4.4440e-02,  4.9545e-02,  2.8287e-01, -4.7697e-02, -4.2282e-01,\n",
       "         9.5420e-02,  1.9307e-01, -1.1203e-01, -9.0357e-02, -2.9291e-01,\n",
       "         1.2004e-02,  6.2639e-01,  1.9446e-01, -7.2232e-02,  1.0573e-01,\n",
       "         2.6316e-01, -7.8862e-02,  2.0491e-01, -1.5756e-01,  9.1737e-02,\n",
       "         6.1741e-03,  1.7387e-02, -2.4001e-01,  6.2160e-01, -1.7076e-01,\n",
       "         1.9036e-01,  2.8525e-01, -1.0839e-01, -2.1672e-01, -1.0496e-01,\n",
       "        -2.4199e-01, -3.7375e-01, -8.3494e-03, -1.4172e-01,  2.7732e-01,\n",
       "        -4.2241e-02,  1.6150e-01, -1.4870e-01, -1.4158e-01,  2.9447e-02,\n",
       "        -4.3514e-02, -2.5469e-02,  3.7050e-02, -1.4405e-03,  7.5802e-02,\n",
       "         8.4546e+01, -2.1155e-01, -1.1452e-01, -2.7569e-01, -1.0924e-01,\n",
       "         2.7238e-01,  5.8817e-02,  6.4638e-02, -2.2884e-01, -9.7181e-02,\n",
       "        -3.5536e-02, -1.2293e-01,  2.2092e+00,  5.8771e-02, -1.6549e-01,\n",
       "         9.9767e-02,  1.4447e-01,  1.1188e+00,  4.8347e-02,  2.5414e-01,\n",
       "        -2.5817e-01, -2.6179e-01,  2.1892e-01, -3.5516e-01,  3.7201e-04,\n",
       "         5.3596e-04, -1.3960e-01,  1.2249e-01, -1.0248e-01, -2.2561e-01,\n",
       "         1.9228e-01,  6.1559e-02,  1.4104e-02, -8.9893e-03,  1.9902e-01,\n",
       "        -3.2487e-01,  2.2997e-01, -2.5298e-02, -1.0476e-01, -7.1718e-02,\n",
       "         5.6724e-02,  8.6032e-04, -2.1641e-01,  2.9166e-01,  1.9637e-01,\n",
       "        -1.5926e-01,  1.0213e-01, -1.0227e-02, -1.0678e-01, -6.1057e+00,\n",
       "        -8.9026e-01,  1.1277e+00, -1.7850e-02, -3.1297e-01,  8.9086e-03,\n",
       "         2.2832e-01,  7.2289e-02, -2.9656e-02, -3.8495e-01, -2.5942e-01,\n",
       "         9.6116e-02,  1.2263e-01,  7.2974e-02, -3.2397e-01, -7.4380e-02,\n",
       "        -5.8293e-02,  1.8750e+02, -1.7660e-01, -6.1830e-02, -2.4080e-03,\n",
       "         4.4896e-01, -4.6264e-02,  2.2747e-01, -2.0220e-01,  2.4149e-01,\n",
       "         2.2291e-02, -2.4284e-01,  2.4835e-01, -2.3568e-01, -2.0891e-01,\n",
       "         1.0435e-01,  4.6072e-02,  1.1575e-01, -3.3317e-01, -2.6694e-01,\n",
       "        -4.0621e-01, -1.0022e-02, -6.5460e-02, -2.0652e-02,  4.6553e-02,\n",
       "         1.7676e-01,  9.1303e-02, -5.1320e-03,  4.4997e-02,  3.1173e-02,\n",
       "        -6.1819e-02, -1.1810e-01, -6.8569e-01,  4.0599e-02,  7.9711e-02,\n",
       "        -2.8139e-02,  1.4537e-01,  2.1019e-01, -2.1582e-01, -3.4415e-01,\n",
       "         4.9281e-02,  1.8908e-01,  1.8253e-01, -2.1412e-01,  1.1782e-01,\n",
       "         2.5280e-01,  1.5896e-01,  1.7477e-01, -8.0957e-02, -2.8239e-01,\n",
       "         1.3593e-01,  1.3457e-01, -1.1926e-01, -2.6970e-01, -2.4761e-01,\n",
       "         1.0357e-01,  1.6688e+00,  6.7382e-02,  2.0099e-01, -5.6865e-01,\n",
       "         1.7664e-02,  1.2823e-01, -9.3063e-02, -1.1710e-01, -5.4126e-01,\n",
       "         2.7790e-02,  4.7581e-03, -2.3963e-01, -1.1237e-01, -1.3233e-01,\n",
       "        -1.4922e-01,  5.3987e-01, -8.2113e-02, -2.6786e-01,  6.6062e-02,\n",
       "         2.8179e-02,  9.8126e-02, -1.4861e-01,  5.7144e-02, -3.4978e-01,\n",
       "         1.4107e-01, -7.2210e-02, -3.4664e-01,  1.1227e-02, -2.6866e-01,\n",
       "        -1.7796e-01, -1.3484e-02, -2.6561e-01,  2.4649e-01, -1.6535e-01,\n",
       "        -1.1715e-01, -7.4852e-02, -4.2988e-02,  4.0140e-01, -1.2603e-01,\n",
       "        -1.7550e-01,  1.4616e-01,  2.4090e-01,  1.4883e-01, -7.3643e-02,\n",
       "        -2.5503e-01, -1.2621e-01, -2.9168e-01,  1.2891e-01, -1.0733e-01,\n",
       "        -1.7982e-01, -4.2342e-02,  2.1668e-01, -2.8519e-01,  4.6919e-01,\n",
       "         1.7412e-01, -2.4420e-01,  2.3689e-02,  3.0721e-01, -1.2543e-03,\n",
       "         2.2188e-01,  6.3586e-02, -2.3475e-01,  7.8670e-02, -2.3338e-01,\n",
       "         9.0196e-02,  3.8264e-02, -1.6399e-01,  1.3881e-01,  2.2078e-01,\n",
       "        -1.4040e-01, -3.4341e-01, -2.8755e-03, -4.5636e-03, -3.2589e-01,\n",
       "         3.4382e-01, -4.4732e-02, -3.0713e+00, -4.1956e-02, -1.2172e-01,\n",
       "        -6.9329e-02, -2.1716e-02, -7.8558e-02,  1.4351e-01,  7.9803e-03,\n",
       "        -7.3527e-01, -2.7977e-01,  2.1153e-02, -2.3399e-01,  2.0417e-01,\n",
       "         3.5073e-02,  2.4533e-01, -1.2267e-02, -1.0937e-01,  3.7429e-01,\n",
       "         2.4369e-02, -1.5105e-02, -5.1306e-02,  1.4531e-02,  1.2422e-01,\n",
       "        -1.0989e-01,  1.3219e-01,  4.7470e-02,  1.5703e-01, -7.8251e-02,\n",
       "        -2.3570e-01,  2.1599e-01,  6.3500e-02, -4.9185e-02, -8.3156e-02,\n",
       "        -2.6469e-01,  3.8859e-02, -3.7337e-01, -7.8389e-02, -1.7194e-01,\n",
       "         1.5834e-01, -6.3125e-02, -1.8921e-01,  1.1781e-01,  7.7899e-02,\n",
       "         1.0662e-02,  1.5518e-02, -9.3918e-02, -9.7878e-02, -1.5972e-01,\n",
       "         1.7222e-01, -8.0307e-02,  5.8828e-02,  1.9975e-01,  1.5407e+00,\n",
       "         9.2758e-02, -2.8329e-01, -5.7540e-01,  1.5496e-01,  9.4463e-02,\n",
       "         1.5275e-01,  8.3438e-02,  1.7325e-01,  1.8821e-01, -1.2761e-01,\n",
       "        -2.9503e-02,  3.1971e-02, -2.3153e-01, -1.0132e-01, -1.0446e-01,\n",
       "         1.9702e-02,  2.3347e-01, -2.1833e-02,  1.0739e-01,  3.9728e-01,\n",
       "        -1.2165e-01,  9.5923e-02, -5.4677e-02, -1.6382e+00, -1.4395e-01,\n",
       "         3.7534e-04, -1.7367e-01, -7.3832e-02, -6.3941e-02, -2.9362e-01,\n",
       "        -2.1780e-02, -3.4110e-02, -1.6010e-01, -2.2738e-01, -2.3166e-02,\n",
       "        -1.2120e-01, -2.6434e-01,  1.1009e-01,  1.2566e+00,  3.6448e-02,\n",
       "         2.2301e-01, -9.1212e-02,  8.6324e-02, -3.7927e-01,  2.2950e-01,\n",
       "         6.4094e-02,  2.1635e-01,  4.2914e-01, -2.9015e-02, -1.5694e-02,\n",
       "        -2.7861e-02, -1.1859e-02, -2.9159e-01, -5.6975e-02,  1.1247e-01,\n",
       "         1.6771e-01,  2.4513e-01,  8.5669e-02,  6.6191e-02,  6.5837e-02,\n",
       "        -2.2634e-01, -1.9871e-01,  2.0034e-01,  3.2967e-01, -2.5276e-01,\n",
       "        -3.9000e-01, -5.4660e-01, -7.3506e-02, -2.9692e-01, -5.0176e-02,\n",
       "        -1.4197e-01,  4.0642e-02, -6.7191e-02, -2.0862e-01,  7.3998e-02,\n",
       "        -2.0471e-01, -5.8851e-01, -2.3492e-01, -1.2244e-02,  8.6530e-02,\n",
       "        -2.3303e-01,  2.3965e-01,  2.6081e+00,  1.7722e-01,  7.3574e-03,\n",
       "         4.0236e-02,  1.5184e-01, -6.8909e-02])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2').to('mps')\n",
    "model.eval()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_embedding_tensor(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to('mps')\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    token_embeddings = token_embeddings.mean(dim=1).squeeze()\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # First layer goes from 768 -> 256\n",
    "        # Second layer goes from 256 -> 64\n",
    "        # Third layer goes from 64 -> 16\n",
    "        # Fourth layer goes from 16 -> 1\n",
    "\n",
    "        self.layer1 = nn.Linear(768, 256, bias=True)\n",
    "        self.layer2 = nn.Linear(256, 64, bias=True)\n",
    "        self.layer3 = nn.Linear(64, 16, bias=True)\n",
    "        self.layer4 = nn.Linear(16, 1, bias=True)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.layers = nn.Sequential(self.layer1, self.activation,\n",
    "                                    self.layer2, self.activation,\n",
    "                                    self.layer3, self.activation,\n",
    "                                    self.layer4, nn.Sigmoid())        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 768)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train']\n",
    "dataset_test = dataset['test']\n",
    "dataset_validation = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset_validation, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Model(\n",
      "  (layer1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (layer2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (layer3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (layer4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = Model()\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "mlp.to(device)\n",
    "\n",
    "print(device)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = AdamW(mlp.parameters(), lr=0.001)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader=train_loader, model=mlp, loss_fn=criterion, optimizer=optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch in iter(dataloader):\n",
    "\n",
    "        # print(\"xxxx\")\n",
    "        # print(batch)\n",
    "        # print(\"xxxx\")\n",
    "\n",
    "        # Data is idx, sentence, label\n",
    "        text = batch['sentence']\n",
    "        label = batch['label']\n",
    "\n",
    "        # print(text)\n",
    "        # print(label)\n",
    "\n",
    "        X = get_embedding_tensor(text)\n",
    "        X = X.to(device)\n",
    "        y = label.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        pred = pred.squeeze(1)\n",
    "\n",
    "        print(y)\n",
    "        print(pred)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss.item()\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_loop(dataloader=val_loader, model=mlp, loss_fn=criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            val_loss += loss_fn(pred, y.to(device)).item()\n",
    "\n",
    "    val_loss /= size\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "early_stop_ctr = 0\n",
    "curr_val_loss = 10000\n",
    "prev_val_loss = 10000\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if early_stop_ctr > 5:\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    train_loss = train_loop()\n",
    "    val_loss = validation_loop()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < prev_val_loss:\n",
    "        early_stop_ctr = 0\n",
    "    else:\n",
    "        early_stop_ctr += 1\n",
    "\n",
    "    prev_val_loss = curr_val_loss\n",
    "    curr_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
